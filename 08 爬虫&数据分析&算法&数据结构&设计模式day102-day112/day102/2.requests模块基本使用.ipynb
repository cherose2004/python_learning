{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 答辩\n",
    "- 需求文档\n",
    "    - 项目背景介绍\n",
    "    - 分工\n",
    "    - 功能模块的介绍\n",
    "    - 业务逻辑\n",
    "- 数据量级: >= 10000\n",
    "- 设计业务逻辑(数据分析+机器学习)\n",
    "- 数据分类:\n",
    "    - 电商\n",
    "    - 新闻资讯\n",
    "    - 房产,招聘\n",
    "    - 医疗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- requests模块\n",
    "    - 概念:一个机遇网络请求的模块.作用就是用来模拟浏览器发起请求.\n",
    "    - 编码流程:\n",
    "        - 指定url\n",
    "        - 进行请求的发送\n",
    "        - 获取响应数据(爬取到的数据)\n",
    "        - 持久化存储\n",
    " - 环境的安装:\n",
    "     - pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取搜狗首页对应的页面源码数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_1\n",
    "url = 'https://www.sogou.com'\n",
    "#step_2:返回值是一个响应对象\n",
    "response = requests.get(url=url)\n",
    "#step_3:text返回的是字符串形式的响应数据\n",
    "page_text = response.text\n",
    "#step_4\n",
    "with open('./sogou.html','w',encoding='utf-8') as fp:\n",
    "    fp.write(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于搜狗编写一个简易的网页采集器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a key:周杰伦\n",
      "周杰伦 下载成功!\n"
     ]
    }
   ],
   "source": [
    "wd = input('enter a key:')\n",
    "url = 'https://www.sogou.com/web'\n",
    "#存储的就是动态的请求参数\n",
    "params = {\n",
    "    'query':wd\n",
    "}\n",
    "#一定需要将params作用到请求中\n",
    "#params参数表示的是对请求url参数的封装\n",
    "response = requests.get(url=url,params=params)\n",
    "\n",
    "page_text = response.text\n",
    "fileName = wd+'.html'\n",
    "with open(fileName,'w',encoding='utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "print(wd,'下载成功!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上述程序出现了问题\n",
    "    - 问题1:爬取到的数据出现了乱码\n",
    "    - 问题2:遇到了UA检测这种反爬机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a key:周杰伦\n",
      "周杰伦 下载成功!\n"
     ]
    }
   ],
   "source": [
    "#解决中文乱码\n",
    "wd = input('enter a key:')\n",
    "url = 'https://www.sogou.com/web'\n",
    "#存储的就是动态的请求参数\n",
    "params = {\n",
    "    'query':wd\n",
    "}\n",
    "#一定需要将params作用到请求中\n",
    "#params参数表示的是对请求url参数的封装\n",
    "response = requests.get(url=url,params=params)\n",
    "\n",
    "#手动修改响应数据的编码\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "page_text = response.text\n",
    "fileName = wd+'.html'\n",
    "with open(fileName,'w',encoding='utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "print(wd,'下载成功!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反爬机制:UA检测\n",
    "- 反反爬机制:UA伪装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a key:波晓张\n",
      "波晓张 下载成功!\n"
     ]
    }
   ],
   "source": [
    "#解决中文乱码&UA伪装\n",
    "wd = input('enter a key:')\n",
    "url = 'https://www.sogou.com/web'\n",
    "#存储的就是动态的请求参数\n",
    "params = {\n",
    "    'query':wd\n",
    "}\n",
    "\n",
    "#即将发起请求对应的头信息\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "#一定需要将params作用到请求中\n",
    "#params参数表示的是对请求url参数的封装\n",
    "#headers参数是用来实现UA伪装\n",
    "response = requests.get(url=url,params=params,headers=headers)\n",
    "\n",
    "#手动修改响应数据的编码\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "page_text = response.text\n",
    "fileName = wd+'.html'\n",
    "with open(fileName,'w',encoding='utf-8') as fp:\n",
    "    fp.write(page_text)\n",
    "print(wd,'下载成功!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取豆瓣电影的电影详情数据\n",
    "    - 分析:当滚轮滑动到底部的时候,发起了一个ajax的请求,且该请求请求到了一组电影数据\n",
    "- 动态加载的数据:就是通过另一个额外的请求请求到的数据\n",
    "    - ajax生成动态加载的数据\n",
    "    - js生成动态加载的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a start:1\n",
      "enter a limit10\n",
      "指环王3：王者无敌 爬取成功\n",
      "七武士 爬取成功\n",
      "蝙蝠侠：黑暗骑士 爬取成功\n",
      "搏击俱乐部 爬取成功\n",
      "指环王1：魔戒再现 爬取成功\n",
      "指环王2：双塔奇兵 爬取成功\n",
      "攻壳机动队 爬取成功\n",
      "将军号 爬取成功\n",
      "V字仇杀队 爬取成功\n",
      "黑客帝国 爬取成功\n"
     ]
    }
   ],
   "source": [
    "url = 'https://movie.douban.com/j/chart/top_list'\n",
    "start = input('enter a start:')\n",
    "limit = input('enter a limit')\n",
    "#处理请求参数\n",
    "params = {\n",
    "    'type': '5',\n",
    "    'interval_id': '100:90',\n",
    "    'action': '',\n",
    "    'start': start,\n",
    "    'limit': limit,\n",
    "}\n",
    "\n",
    "response = requests.get(url=url,params=params,headers=headers)\n",
    "#json返回的是序列化好的对象\n",
    "data_list = response.json()\n",
    "\n",
    "fp = open('douban.txt','w',encoding='utf-8')\n",
    "\n",
    "for dic in data_list:\n",
    "    name = dic['title']\n",
    "    score = dic['score']\n",
    "    \n",
    "    fp.write(name+':'+score+'\\n')\n",
    "    print(name,'爬取成功')\n",
    " \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取肯德基餐厅位置信息http://www.kfc.com.cn/kfccda/storelist/index.aspx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a city name:北京\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Table': [{'rowcount': 73}],\n",
       " 'Table1': [{'addressDetail': '北京经济开发区西环北路18号F1＋F2',\n",
       "   'cityName': '北京市',\n",
       "   'pro': '24小时,Wi-Fi,店内参观,礼品卡,生日餐会',\n",
       "   'provinceName': '北京市',\n",
       "   'rownum': 5,\n",
       "   'storeName': '亦庄'},\n",
       "  {'addressDetail': '通顺路石园西区南侧北京顺义西单商场石园分店一层、二层部分',\n",
       "   'cityName': '北京市',\n",
       "   'pro': '24小时,Wi-Fi,店内参观,礼品卡',\n",
       "   'provinceName': '北京市',\n",
       "   'rownum': 6,\n",
       "   'storeName': '石园南大街'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'\n",
    "city = input('enter a city name:')\n",
    "data = {\n",
    "    'cname': '',\n",
    "    'pid': '',\n",
    "    'keyword': city,\n",
    "    'pageIndex': '3',\n",
    "    'pageSize': '2',\n",
    "}\n",
    "#data参数表示的就是get方法中的params\n",
    "response = requests.post(url=post_url,data=data,headers=headers)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考:如何判定一张页面中是否存在动态加载的数据\n",
    "- 抓包工具进行局部搜索\n",
    "- 如果判定出页面中有动态加载的数据如何进行数据的定位?\n",
    "    - 使用抓包工具进行全局搜索\n",
    "- 对一个陌生的网站数据进行爬取前一定要判定你爬取的数据是否为动态加载的!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取企业详情信息:http://125.35.6.84:81/xk/\n",
    "- 分析:\n",
    "    - 1.网站的首页和企业的详情页的数据都是动态加载出来的\n",
    "    - 2.分析某一家企业的企业详情数据是怎么来的?\n",
    "        - 企业详情数据时通过一个ajax请求(post)请求到的.\n",
    "        - 请求对应的url:http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById\n",
    "        - 该请求携带了一个参数:id:xxdxxxx\n",
    "        - 结论:\n",
    "            - 1.每家企业详情页的数据都是通过一个post形式的ajax请求请求到的\n",
    "            - 2.每家企业对应的ajax请求的url都一样,请求方式都是post,只有请求参数id的值不一样.\n",
    "            - 3.只需要获取每一家企业对应的id值即可获取每一家企业对应的详情数据\n",
    "    - 需要获取每一家企业的id值\n",
    "        - 思路:每一家企业的id值应该存储在首页对应的相关请求或者响应中.\n",
    "        - 结论:每一家企业的id值是存储在首页中的某一个ajax请求对应的响应数据中,只需要将该响应数据中企业的id提取/解析出来后即可."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江苏正东生物科技有限公司 爬取成功!!!\n",
      "通化昌源医药科技有限公司 爬取成功!!!\n",
      "启励（广州）生物科技有限公司 爬取成功!!!\n",
      "广州真事美化妆品制造有限公司 爬取成功!!!\n",
      "广州市暨鼎生物科技有限公司 爬取成功!!!\n",
      "西安尹千容生物科技有限责任公司 爬取成功!!!\n",
      "施洛丹（福建）工贸有限公司 爬取成功!!!\n",
      "莱芜瑶草生物科技有限公司 爬取成功!!!\n",
      "辽宁婵泉生物药业有限公司 爬取成功!!!\n",
      "辽宁东宁药业有限公司 爬取成功!!!\n",
      "广州二天堂制药有限公司 爬取成功!!!\n",
      "赫莲娜（广州）生物科技有限公司 爬取成功!!!\n",
      "广州腾信生物科技有限公司 爬取成功!!!\n",
      "广州娜艾施化妆品有限公司 爬取成功!!!\n",
      "广州汉方医学生物科技有限公司 爬取成功!!!\n"
     ]
    }
   ],
   "source": [
    "#要请求到没一家企业对应的id\n",
    "url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList'\n",
    "data = {\n",
    "    'on': 'true',\n",
    "    'page': '1',\n",
    "    'pageSize': '15',\n",
    "    'productName': '',\n",
    "    'conditionType': '1',\n",
    "    'applyname': '',\n",
    "    'applysn': '',\n",
    "}\n",
    "\n",
    "fp = open('./company_detail.txt','w',encoding='utf-8')\n",
    "\n",
    "#该json()的返回值中就有每一家企业的id\n",
    "data_dic = requests.post(url=url,data=data,headers=headers).json()\n",
    "#解析id\n",
    "for dic in data_dic['list']:\n",
    "    _id = dic['ID']\n",
    "#     print(_id)\n",
    "    #对每一个id对应的企业详情数据进行捕获(发起请求)\n",
    "    post_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById'\n",
    "    post_data = {\n",
    "        'id':_id\n",
    "    }\n",
    "    #json的返回值是某一家企业的详情信息\n",
    "    detail_dic = requests.post(url=post_url,data=post_data,headers=headers).json()\n",
    "    company_title = detail_dic['epsName']\n",
    "    address = detail_dic['epsProductAddress']\n",
    "    \n",
    "    fp.write(company_title+':'+address+'\\n')\n",
    "    print(company_title,'爬取成功!!!')\n",
    "fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是抓包工具\n",
    "    - 代理服务器\n",
    "        - 作用:进行请求转发的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
