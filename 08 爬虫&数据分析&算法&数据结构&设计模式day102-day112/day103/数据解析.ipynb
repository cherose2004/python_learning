{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回顾\n",
    "- 爬虫\n",
    "- 爬虫的分类:\n",
    "    - 通用\n",
    "    - 聚焦\n",
    "    - 增量式:监测\n",
    "- 反爬机制\n",
    "- 反反爬策略\n",
    "- robots,UA监测:UA伪装\n",
    "- http&https概念:服务器和客户端进行数据交互的某种形式\n",
    "- 常用的头信息:\n",
    "    - User-Agent:请求载体的身份标识\n",
    "    - Connection:close\n",
    "    - content-type\n",
    "- https的加密方式:证书秘钥加密\n",
    "    - 证书:是被应用在https的加密操作中的.该证书是有证书认证机构颁布的,证书中包含了公钥(加密方式)\n",
    "- requests:\n",
    "    - get/post:\n",
    "        - url\n",
    "        - data/params:对请求参数的封装\n",
    "        - headers:UA伪装\n",
    "     - 什么是动态加载的数据:由另一个额外的请求请求到的数据\n",
    "         - ajax\n",
    "         - js\n",
    "     - 如何鉴定页面中是否有动态加载的数据?\n",
    "         - 局部搜索\n",
    "         - 全局搜索\n",
    "     - 对一个陌生网站进行爬取前的第一步做什么?\n",
    "         - 确定你要爬取的数据是否为动态加载的!!!\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江苏正东生物科技有限公司 爬取成功!!!\n",
      "通化昌源医药科技有限公司 爬取成功!!!\n",
      "启励（广州）生物科技有限公司 爬取成功!!!\n",
      "广州真事美化妆品制造有限公司 爬取成功!!!\n",
      "广州市暨鼎生物科技有限公司 爬取成功!!!\n",
      "西安尹千容生物科技有限责任公司 爬取成功!!!\n",
      "施洛丹（福建）工贸有限公司 爬取成功!!!\n",
      "莱芜瑶草生物科技有限公司 爬取成功!!!\n",
      "辽宁婵泉生物药业有限公司 爬取成功!!!\n",
      "辽宁东宁药业有限公司 爬取成功!!!\n",
      "广州二天堂制药有限公司 爬取成功!!!\n",
      "赫莲娜（广州）生物科技有限公司 爬取成功!!!\n",
      "广州腾信生物科技有限公司 爬取成功!!!\n",
      "广州娜艾施化妆品有限公司 爬取成功!!!\n",
      "广州汉方医学生物科技有限公司 爬取成功!!!\n"
     ]
    }
   ],
   "source": [
    "#作业\n",
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "}\n",
    "fp = open('./company_detail.txt','w',encoding='utf-8')\n",
    "for page in range(1,6):\n",
    "    #要请求到没一家企业对应的id\n",
    "    url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList'\n",
    "    data = {\n",
    "        'on': 'true',\n",
    "        'page': str(page),\n",
    "        'pageSize': '15',\n",
    "        'productName': '',\n",
    "        'conditionType': '1',\n",
    "        'applyname': '',\n",
    "        'applysn': '',\n",
    "    }\n",
    "\n",
    "\n",
    "    #该json()的返回值中就有每一家企业的id\n",
    "    data_dic = requests.post(url=url,data=data,headers=headers).json()\n",
    "    #解析id\n",
    "    for dic in data_dic['list']:\n",
    "        _id = dic['ID']\n",
    "    #     print(_id)\n",
    "        #对每一个id对应的企业详情数据进行捕获(发起请求)\n",
    "        post_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById'\n",
    "        post_data = {\n",
    "            'id':_id\n",
    "        }\n",
    "        #json的返回值是某一家企业的详情信息\n",
    "        detail_dic = requests.post(url=post_url,data=post_data,headers=headers).json()\n",
    "        company_title = detail_dic['epsName']\n",
    "        address = detail_dic['epsProductAddress']\n",
    "\n",
    "        fp.write(company_title+':'+address+'\\n')\n",
    "        print(company_title,'爬取成功!!!')\n",
    "fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据解析\n",
    "- 解析:根据指定的规则对数据进行提取\n",
    "- 作用:实现聚焦爬虫\n",
    "- 聚焦爬虫的编码流程:\n",
    "    - 指定url\n",
    "    - 发起请求\n",
    "    - 获取响应数据\n",
    "    - 数据解析\n",
    "    - 持久化存储\n",
    "- 数据解析的方式:\n",
    "    - 正则\n",
    "    - bs4\n",
    "    - xpath\n",
    "    - pyquery(拓展)\n",
    "- 数据解析的通用原理是什么?\n",
    "    - 数据解析需要作用在页面源码中(一组html标签组成的)\n",
    "    - html的核心作用是什么?\n",
    "        - 展示数据\n",
    "    - html是如何展示数据的呢?\n",
    "        - html所要展示的数据一定是被放置在html标签之中,或者是在属性中.\n",
    "    - 通用原理:\n",
    "        - 1.标签定位\n",
    "        - 2.取文本or取属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则实现的数据解析\n",
    "- 需求:爬取糗事百科中糗图数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何爬取图片数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式1:\n",
    "url = 'https://pic.qiushibaike.com/system/pictures/12217/122176396/medium/OM37E794HBL3OFFF.jpg'\n",
    "img_data = requests.get(url=url,headers=headers).content #content返回的是byte类型的数据\n",
    "with open('./123.jpg','wb') as fp:\n",
    "    fp.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./456.jpg', <http.client.HTTPMessage at 0x1deee01c470>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#方式2:\n",
    "from urllib import request\n",
    "url = 'https://pic.qiushibaike.com/system/pictures/12217/122176396/medium/OM37E794HBL3OFFF.jpg'\n",
    "request.urlretrieve(url,'./456.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方式1和方式2对于图片数据爬取的操作最大的不同之处是在哪?\n",
    "    - 方式2不可以使用UA伪装的机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- urllib就是一个比较老的网络请求的模块,在requests模块没有出现之前,请求发送的操作使用的都是urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I844CD5W6MBJHYYG.jpg 下载成功!!!\n",
      "EKOKK8LVTIR1N2MH.jpg 下载成功!!!\n",
      "FDK0KFSSECW4DI96.jpg 下载成功!!!\n",
      "KVN1L0WEZCF0MP26.jpg 下载成功!!!\n",
      "U0EW7EK1CL4U4WOK.jpg 下载成功!!!\n",
      "D106TGNQGR5LFF2U.jpg 下载成功!!!\n",
      "UVF5CP3W9EDEOE9J.jpg 下载成功!!!\n",
      "753IO5IUKOQIGCCF.jpg 下载成功!!!\n",
      "7X5DH4Z5QUJ06DA7.jpg 下载成功!!!\n",
      "OM37E794HBL3OFFF.jpg 下载成功!!!\n",
      "XBOP3Y2YQM1SEEXA.jpg 下载成功!!!\n",
      "BI0L7EQORAWQUPG4.jpg 下载成功!!!\n",
      "8UOZ13Y20PU1LUAM.jpg 下载成功!!!\n",
      "GYFB658QZ2U281T0.jpg 下载成功!!!\n",
      "WNKXU7AIUKHM81JN.jpg 下载成功!!!\n",
      "CFO6WX11MT8040XV.jpg 下载成功!!!\n",
      "J9396ISKSP88LN85.jpg 下载成功!!!\n",
      "U7XOWOHFA2II936K.jpg 下载成功!!!\n",
      "I94DJC62BQDD8DTN.jpg 下载成功!!!\n",
      "2VRQ8EA1GT6TME3U.jpg 下载成功!!!\n"
     ]
    }
   ],
   "source": [
    "#糗事百科\n",
    "import re\n",
    "import os\n",
    "\n",
    "dir_name = './qiutuLibs'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "url = 'https://www.qiushibaike.com/pic/'\n",
    "page_text = requests.get(url,headers=headers).text\n",
    "#数据解析:图片地址\n",
    "ex = '<div class=\"thumb\">.*?<img src=\"(.*?)\" alt=.*?</div>'\n",
    "img_src_list = re.findall(ex,page_text,re.S)\n",
    "for src in img_src_list:\n",
    "    src = 'https:'+src\n",
    "    img_name = src.split('/')[-1]\n",
    "    img_path = dir_name+'/'+img_name\n",
    "    #对图片地址单独发起请求获取图片数据\n",
    "    request.urlretrieve(src,img_path)\n",
    "    print(img_name,'下载成功!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取多页\n",
    "- 分析:每一个页码对应的url是有共性:https://www.qiushibaike.com/pic/page/%d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取第1页的图片\n",
      "正在爬取第2页的图片\n",
      "正在爬取第3页的图片\n",
      "正在爬取第4页的图片\n"
     ]
    }
   ],
   "source": [
    "dir_name = './qiutuLibs'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "#指定一个通用的url模板(不可变)\n",
    "url = 'https://www.qiushibaike.com/pic/page/%d/'\n",
    "for page in range(1,5):\n",
    "    print('正在爬取第{}页的图片'.format(page))\n",
    "    #形成一个某页码完整的url\n",
    "    new_url = format(url%page)\n",
    "    page_text = requests.get(new_url,headers=headers).text\n",
    "    #数据解析:图片地址\n",
    "    ex = '<div class=\"thumb\">.*?<img src=\"(.*?)\" alt=.*?</div>'\n",
    "    img_src_list = re.findall(ex,page_text,re.S)\n",
    "    for src in img_src_list:\n",
    "        src = 'https:'+src\n",
    "        img_name = src.split('/')[-1]\n",
    "        img_path = dir_name+'/'+img_name\n",
    "        #对图片地址单独发起请求获取图片数据\n",
    "        request.urlretrieve(src,img_path)\n",
    "#         print(img_name,'下载成功!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bs4解析\n",
    "- 环境的安装:\n",
    "    - pip install bs4\n",
    "    - pip install lxml\n",
    "- bs4的解析原理\n",
    "    - 实例化一个BeautifulSoup的对象,并且将即将被解析的页面源码数据加载到该对象中\n",
    "    - 调用BeautifulSoup对象中的相关属性和方法进行标签定位和数据提取\n",
    "- 如何实例化BeautifulSoup对象呢?\n",
    "    - BeautifulSoup(fp,'lxml'):专门用作于解析本地存储的html文档中的数据\n",
    "    - BeautifulSoup(page_text,'lxml'):专门用作于将互联网上请求到的页面源码数据进行解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标签定位\n",
    "- soup.tagName:定位到第一个TagName标签,返回的是单数\n",
    "- 属性定位:soup.find('tagName',attrName='value'),返回也是单数\n",
    "    - find_all:和find用法一致,但是返回值是列表\n",
    "- 选择器定位:select('选择器'),返回值为列表\n",
    "    - 标签,类,id,层级(>:一个层级,空格:多个层级)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取数据\n",
    "- 取文本:\n",
    "    - tag.string:标签中直系的文本内容\n",
    "    - tag.text:标签中所有的文本内容\n",
    "- 取属性:\n",
    "    - tag['attrName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.haha.com'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fp = open('./test.html','r',encoding='utf-8')\n",
    "soup = BeautifulSoup(fp,'lxml') #将即将被解析的页面源码加载到该对象中\n",
    "soup.p\n",
    "soup.find('div',class_='song')\n",
    "soup.find_all('div',class_='song')\n",
    "soup.select('.tang')\n",
    "soup.select('#feng')\n",
    "soup.select('.tang > ul > li')\n",
    "soup.select('.tang li')\n",
    "li_6 = soup.select('.tang > ul > li')[6]\n",
    "i_tag = li_6.i\n",
    "i_tag.string\n",
    "soup.find('div',class_='tang').text\n",
    "soup.find('a',id=\"feng\")['href']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬取三国演义整篇小说内容http://www.shicimingju.com/book/sanguoyanyi.html\n",
    "    - 章节名称\n",
    "    - 章节内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一回·宴桃园豪杰三结义  斩黄巾英雄首立功 爬取成功!\n",
      "第二回·张翼德怒鞭督邮    何国舅谋诛宦竖 爬取成功!\n",
      "第三回·议温明董卓叱丁原  馈金珠李肃说吕布 爬取成功!\n",
      "第四回·废汉帝陈留践位    谋董贼孟德献刀 爬取成功!\n",
      "第五回·发矫诏诸镇应曹公  破关兵三英战吕布 爬取成功!\n",
      "第六回·焚金阙董卓行凶    匿玉玺孙坚背约 爬取成功!\n",
      "第七回·袁绍磐河战公孙    孙坚跨江击刘表 爬取成功!\n",
      "第八回·王司徒巧使连环计  董太师大闹凤仪亭 爬取成功!\n",
      "第九回·除暴凶吕布助司徒  犯长安李傕听贾诩 爬取成功!\n",
      "第一十回·勤王室马腾举义    报父仇曹操兴师 爬取成功!\n",
      "第十一回·刘皇叔北海救孔融  吕温侯濮阳破曹操 爬取成功!\n",
      "第十二回·陶恭祖三让徐州    曹孟德大战吕布 爬取成功!\n",
      "第十三回·李傕郭汜大交兵  杨奉董承双救驾 爬取成功!\n",
      "第十四回·曹孟德移驾幸许都  吕奉先乘夜袭徐郡 爬取成功!\n",
      "第十五回·太史慈酣斗小霸王  孙伯符大战严白虎 爬取成功!\n",
      "第十六回·吕奉先射戟辕门    曹孟德败师淯水 爬取成功!\n",
      "第十七回·袁公路大起七军    曹孟德会合三将 爬取成功!\n",
      "第十八回·贾文和料敌决胜    夏侯惇拔矢啖睛 爬取成功!\n",
      "第十九回·下邳城曹操鏖兵    白门楼吕布殒命 爬取成功!\n",
      "第二十回·曹阿瞒许田打围    董国舅内阁受诏 爬取成功!\n",
      "第二十一回·曹操煮酒论英雄  关公赚城斩车胄 爬取成功!\n",
      "第二十二回·袁曹各起马步三军  关张共擒王刘二将 爬取成功!\n",
      "第二十三回·祢正平裸衣骂贼    吉太医下毒遭刑 爬取成功!\n",
      "第二十四回·国贼行凶杀贵妃    皇叔败走投袁绍 爬取成功!\n",
      "第二十五回·屯土山关公约三事  救白马曹操解重围 爬取成功!\n",
      "第二十六回·袁本初败兵折将    关云长挂印封金 爬取成功!\n",
      "第二十七回·美髯公千里走单骑  汉寿侯五关斩六将 爬取成功!\n",
      "第二十八回·斩蔡阳兄弟释疑    会古城主臣聚义 爬取成功!\n",
      "第二十九回·小霸王怒斩于吉    碧眼儿坐领江东 爬取成功!\n",
      "第三十回·战官渡本初败绩  劫乌巢孟德烧粮 爬取成功!\n",
      "第三十一回·曹操仓亭破本初    玄德荆州依刘表 爬取成功!\n",
      "第三十二回·夺冀州袁尚争锋    决漳河许攸献计 爬取成功!\n",
      "第三十三回·曹丕乘乱纳甄氏    郭嘉遗计定辽东 爬取成功!\n",
      "第三十四回·蔡夫人隔屏听密语  刘皇叔跃马过檀溪 爬取成功!\n",
      "第三十五回·玄德南漳逢隐沧    单福新野遇英主 爬取成功!\n",
      "第三十六回·玄德用计袭樊城    元直走马荐诸葛 爬取成功!\n",
      "第三十七回·司马徽再荐名士    刘玄德三顾草庐 爬取成功!\n",
      "第三十八回·定三分隆中决策    战长江孙氏报仇 爬取成功!\n",
      "第三十九回·荆州城公子三求计  博望坡军师初用兵 爬取成功!\n",
      "第四十回·蔡夫人议献荆州    诸葛亮火烧新野 爬取成功!\n",
      "第四十一回·刘玄德携民渡江    赵子龙单骑救主 爬取成功!\n",
      "第四十二回·张翼德大闹长坂桥  刘豫州败走汉津口 爬取成功!\n",
      "第四十三回·诸葛亮舌战群儒    鲁子敬力排众议 爬取成功!\n",
      "第四十四回·孔明用智激周瑜    孙权决计破曹操 爬取成功!\n",
      "第四十五回·三江口曹操折兵    群英会蒋干中计 爬取成功!\n",
      "第四十六回·用奇谋孔明借箭    献密计黄盖受刑 爬取成功!\n",
      "第四十七回·阚泽密献诈降书    庞统巧授连环计 爬取成功!\n",
      "第四十八回·宴长江曹操赋诗    锁战船北军用武 爬取成功!\n",
      "第四十九回·七星坛诸葛祭风    三江口周瑜纵火 爬取成功!\n",
      "第五十回·诸葛亮智算华容    关云长义释曹操 爬取成功!\n",
      "第五十一回·曹仁大战东吴兵    孔明一气周公瑾 爬取成功!\n",
      "第五十二回·诸葛亮智辞鲁肃    赵子龙计取桂阳 爬取成功!\n",
      "第五十三回·关云长义释黄汉升  孙仲谋大战张文远 爬取成功!\n",
      "第五十四回·吴国太佛寺看新郎  刘皇叔洞房续佳偶 爬取成功!\n",
      "第五十五回·玄德智激孙夫人    孔明二气周公瑾 爬取成功!\n",
      "第五十六回·曹操大宴铜雀台    孔明三气周公瑾 爬取成功!\n",
      "第五十七回·柴桑口卧龙吊丧    耒阳县凤雏理事 爬取成功!\n",
      "第五十八回·马孟起兴兵雪恨    曹阿瞒割须弃袍 爬取成功!\n",
      "第五十九回·许诸裸衣斗马超    曹操抹书问韩遂 爬取成功!\n",
      "第六十回·张永年反难杨修    庞士元议取西蜀 爬取成功!\n",
      "第六十一回·赵云截江夺阿斗    孙权遗书退老瞒 爬取成功!\n",
      "第六十二回·取涪关杨高授首    攻雒城黄魏争功 爬取成功!\n",
      "第六十三回·诸葛亮痛哭庞统    张翼德义释严颜 爬取成功!\n",
      "第六十四回·孔明定计捉张任    杨阜借兵破马超 爬取成功!\n",
      "第六十五回·马超大战葭萌关    刘备自领益州牧 爬取成功!\n",
      "第六十六回·关云长单刀赴会    伏皇后为国捐生 爬取成功!\n",
      "第六十七回·曹操平定汉中地    张辽威震逍遥津 爬取成功!\n",
      "第六十八回·甘宁百骑劫魏营    左慈掷杯戏曹操 爬取成功!\n",
      "第六十九回·卜周易管辂知机    讨汉贼五臣死节 爬取成功!\n",
      "第七十回·猛张飞智取瓦口隘  老黄忠计夺天荡山 爬取成功!\n",
      "第七十一回·占对山黄忠逸待劳  据汉水赵云寡胜众 爬取成功!\n",
      "第七十二回·诸葛亮智取汉中    曹阿瞒兵退斜谷 爬取成功!\n",
      "第七十三回·玄德进位汉中王    云长攻拔襄阳郡 爬取成功!\n",
      "第七十四回·庞令明抬榇决死战  关云长放水淹七军 爬取成功!\n",
      "第七十五回·关云长刮骨疗毒    吕子明白衣渡江 爬取成功!\n",
      "第七十六回·徐公明大战沔水    关云长败走麦城 爬取成功!\n",
      "第七十七回·玉泉山关公显圣    洛阳城曹操感神 爬取成功!\n",
      "第七十八回·治风疾神医身死    传遗命奸雄数终 爬取成功!\n",
      "第七十九回·兄逼弟曹植赋诗    侄陷叔刘封伏法 爬取成功!\n",
      "第八十回·曹丕废帝篡炎刘    汉王正位续大统 爬取成功!\n",
      "第八十一回·急兄仇张飞遇害    雪弟恨先主兴兵 爬取成功!\n",
      "第八十二回·孙权降魏受九锡    先主征吴赏六军 爬取成功!\n",
      "第八十三回·战猇亭先主得仇人  守江口书生拜大将 爬取成功!\n",
      "第八十四回·陆逊营烧七百里    孔明巧布八阵图 爬取成功!\n",
      "第八十五回·刘先主遗诏托孤儿  诸葛亮安居平五路 爬取成功!\n",
      "第八十六回·难张温秦宓逞天辩  破曹丕徐盛用火攻 爬取成功!\n",
      "第八十七回·征南寇丞相大兴师  抗天兵蛮王初受执 爬取成功!\n",
      "第八十八回·渡泸水再缚番王    识诈降三擒孟获 爬取成功!\n",
      "第八十九回·武乡侯四番用计    南蛮王五次遭擒 爬取成功!\n",
      "第九十回·驱巨善六破蛮兵    烧藤甲七擒孟获 爬取成功!\n",
      "第九十一回·祭泸水汉相班师    伐中原武侯上表 爬取成功!\n",
      "第九十二回·赵子龙力斩五将    诸葛亮智取三城 爬取成功!\n",
      "第九十三回·姜伯约归降孔明    武乡侯骂死王朝 爬取成功!\n",
      "第九十四回·诸葛亮乘雪破羌兵  司马懿克日擒孟达 爬取成功!\n",
      "第九十五回·马谡拒谏失街亭    武侯弹琴退仲达 爬取成功!\n",
      "第九十六回·孔明挥泪斩马谡    周鲂断发赚曹休 爬取成功!\n",
      "第九十七回·讨魏国武侯再上表  破曹兵姜维诈献书 爬取成功!\n",
      "第九十八回·追汉军王双受诛    袭陈仓武侯取胜 爬取成功!\n",
      "第九十九回·诸葛亮大破魏兵    司马懿入寇西蜀 爬取成功!\n",
      "第一百回·汉兵劫寨破曹真    武侯斗阵辱仲达 爬取成功!\n",
      "第一百十一回·出陇上诸葛妆神    奔剑阁张郃中计 爬取成功!\n",
      "第一百十二回·司马懿占北原渭桥  诸葛亮造木牛流马 爬取成功!\n",
      "第一百十三回·上方谷司马受困    五丈原诸葛禳星 爬取成功!\n",
      "第一百十四回·陨大星汉丞相归天  见木像魏都督丧胆 爬取成功!\n",
      "第一百十五回·武侯预伏锦囊计    魏主拆取承露盘 爬取成功!\n",
      "第一百十六回·公孙渊兵败死襄平  司马懿诈病赚曹爽 爬取成功!\n",
      "第一百十七回·魏主政归司马氏    姜维兵败牛头山 爬取成功!\n",
      "第一百十八回·丁奉雪中奋短兵    孙峻席间施密计 爬取成功!\n",
      "第一百十九回·困司马汉将奇谋    废曹芳魏家果报 爬取成功!\n",
      "第一百一十回·文鸯单骑退雄兵    姜维背水破大敌 爬取成功!\n",
      "第一百一十一回·邓士载智败姜伯约  诸葛诞义讨司马昭 爬取成功!\n",
      "第一百一十二回·救寿春于诠死节    取长城伯约鏖兵 爬取成功!\n",
      "第一百一十三回·丁奉定计斩孙綝    姜维斗阵破邓艾 爬取成功!\n",
      "第一百一十四回·曹髦驱车死南阙    姜维弃粮胜魏兵 爬取成功!\n",
      "第一百一十五回·诏班师后主信谗    托屯田姜维避祸 爬取成功!\n",
      "第一百一十六回·钟会分兵汉中道    武侯显圣定军山 爬取成功!\n",
      "第一百一十七回·邓士载偷度阴平    诸葛瞻战死绵竹 爬取成功!\n",
      "第一百一十八回·哭祖庙一王死孝    入西川二士争功 爬取成功!\n",
      "第一百一十九回·假投降巧计成虚话  再受禅依样画葫芦 爬取成功!\n",
      "第一百二十回·荐杜预老将献新谋  降孙皓三分归一统 爬取成功!\n"
     ]
    }
   ],
   "source": [
    "#在首页中解析章节名称&每一个章节详情页的url\n",
    "url = 'http://www.shicimingju.com/book/sanguoyanyi.html'\n",
    "page_text = requests.get(url,headers=headers).text\n",
    "soup = BeautifulSoup(page_text,'lxml')\n",
    "a_list = soup.select('.book-mulu > ul > li > a')\n",
    "fp = open('sanguo.txt','w',encoding='utf-8')\n",
    "for a in a_list:\n",
    "    detail_url = 'http://www.shicimingju.com'+a['href']\n",
    "    chap_title = a.string\n",
    "    #对章节详情页的url发起请求,解析详情页中的章节内容\n",
    "    detail_page_text = requests.get(detail_url,headers=headers).text\n",
    "    soup = BeautifulSoup(detail_page_text,'lxml')\n",
    "    chap_content = soup.find('div',class_=\"chapter_content\").text\n",
    "    fp.write(chap_title+':'+chap_content+'\\n')\n",
    "    print(chap_title,'爬取成功!')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xpath解析\n",
    "- 环境的安装:pip install lxml\n",
    "- xpath的解析原理\n",
    "    - 实例化一个etree类型的对象,且将页面源码数据加载到该对象中\n",
    "    - 需要调用该对象的xpath方法结合着不同形式的xpath表达式进行标签定位和数据提取\n",
    "- etree对象的实例化\n",
    "    - etree.parse(fileNane)\n",
    "    - etree.HTML(page_text)\n",
    "- xpath方法返回的永远是一个列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标签定位\n",
    "- 在xpath表达式中最最侧的/表示的含义是说,当前定位的标签必须从根节点开始进行定位\n",
    "- xpath表达式中最左侧的//表示可以从任意位置进行标签定位\n",
    "- xpath表达式中非最左侧的//表示的是多个层级的意思\n",
    "- xpath表达式中非最左侧的/表示的是一个层级的意思\n",
    "- 属性定位://tagName[@arrtName='value']\n",
    "- 索引定位://tagName/li[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取数据\n",
    "- 取文本:\n",
    "    - /text():取直系的文本内容\n",
    "    - //text():取所有的文本内容\n",
    "- 取属性:\n",
    "    - tag/@attrName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.haha.com']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "tree = etree.parse('./test.html')\n",
    "tree.xpath('/html/head/meta')[0] #绝对路径\n",
    "tree.xpath('//meta')[0] #相对路径,将整个页面源码中所有的meta进行定位\n",
    "tree.xpath('/html//meta')[0] \n",
    "#属性定位\n",
    "tree.xpath('//div[@class=\"song\"]')\n",
    "#索引定位\n",
    "tree.xpath('//div[@class=\"tang\"]/ul/li[3]') #该索引是从1开始\n",
    "tree.xpath('//div[@class=\"tang\"]//li[3]') #该索引是从1开始\n",
    "#取文本\n",
    "tree.xpath('//p[1]/text()')\n",
    "tree.xpath('//div[@class=\"song\"]//text()')\n",
    "\n",
    "#取属性\n",
    "tree.xpath('//a[@id=\"feng\"]/@href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 需求:爬取boss的招聘信息\n",
    "    - 岗位名称\n",
    "    - 公司名称\n",
    "    - 薪资\n",
    "    - 岗位描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',\n",
    "    'cookie':'lastCity=101010100; __c=1566877560; __g=-; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1566877561; _uab_collina=156687756118178796315757; __l=l=%2Fwww.zhipin.com%2F&r=https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DidbSvNzz2fLSl1WXiEmtINauVHUZYSNqejHny725pc5RTwaHqh5uDx1LewpyGmaT%26wd%3D%26eqid%3Dbadf667700040677000000025d64a772&friend_source=0&friend_source=0; __zp_stoken__=91d9QItKEtUk5dMMnDG7lwzq8mBW1g%2FkEsFOHXIi%2FwMd%2FPRRXc%2FPMKjsDYwsfC4b7vAT3FVnTmYBjGp8gW1OeZ5TdA%3D%3D; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1566879753; __a=69160831.1566877560..1566877560.16.1.16.16'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python爬虫工程师 11-20K 脉讯在线 \n",
      "                                    岗位职责：负责大规模网站的抓取、提取、质量识别与垃圾过滤、更新频率控制等工作。任职要求：1、有扎实的数据结构和算法功底；2、工作认真细致踏实，有较强的学习能力 ；3、熟悉linux开发环境，熟悉python、C++或Java语言 ；4、理解http，熟悉html, DOM, xpath, scrapy优先 ；5、有爬虫，信息抽取，文本分类相关经验者优先 ；6、熟悉Kafka、Elasticsearch优先；\n",
      "                                脉讯2009年成立，是传播大数据公司，是舆情监测、传播管理行业领先者。为世界500强、上市公司、政府提供专业的内容数据监测、分析传播管理服务。技术是脉讯的核心驱动力，基于数据的服务能力是脉讯的关键能力，通过数据监测技术、智能分析技术、专业分析能力和有价值的社交聆听，帮助客户做出商业决策\n",
      "                                        【脉讯介绍】关于脉讯，集合理工男、学院范儿、情怀等标签于一身的CEO Gerry会告诉你，脉讯是——一家设计范儿十足的数据公司，地处中国硅谷之称的北京海淀区，最初是北大计算机系网络实验室的研究项目，10年推出了网络传播监测平台Cyberin，15年推出了自媒体投放平台W\n",
      "                                    \n",
      "Python爬虫开发 8-13K 北京动力天下科技发展有限公司 \n",
      "                                    1、熟悉Python、PHP、C++一种或多种语言,3年以上Python服务器端开发经验。2、精通Scrapy和Pyspider框架进行分布式爬虫，XPath语法，数据分析等能力3、精通Ajax数据爬取，。4、精通定制带有cookie信息的请求头，进行模拟用户登陆。分析网页结构与规律，使用Scrapy爬虫框架和Pyspider框架配合Fiddler工具抓包。使用XPath语法获取需要的信息。5、结合Chrome，Firefox通过Selenium Webdriver获取模拟用户使用指定浏览器发起HTTP访问各阶段耗时及详细资源数据；6、 精通Python服务端编程，精通MapReduce/Hadoop/Hbase/Spark，gevent,socket等。7、熟悉常用算法和数据结构。8、精通Celery等分布式任务队列框架以及消息队列。9、具有良好的自学能力，能通过研究开源项目来解决开发中遇到的难题。10、 熟练使用Linux、及MySQL/MongoDB数据库，有独立搭建大型系统的能力。11、具备良好的编码习惯，有开放平台开发经验者优先。\n",
      "                                \n",
      "                                        2016年开始，我们逐步将早期开源软件转移到MPCloud上来，并对积极参与MPCloud社区活动，支持中国本土的开源社区发展。社区为MPCloud带来了人才、创新、客户。感谢开源带来的全新的商业模式和商业文化，让我们在开源的道路上更加自信。MP CLOUD是MobTop OS开源框架的简称。魔\n",
      "                                    \n",
      "Python爬虫与数据工程师 12-24K·14薪 百观 \n",
      "                                    职位描述：数据工程师将是团队核心成员。我们会一起挑战有趣的技术难题，在自由开放的氛围下，将前沿的数据抓取与存储技术转化成业界独一无二，极具价值的产品。职责：- 探索并实践前沿爬虫技术与存储技术- 分布式爬虫系统的开发，维护，与优化- 对接第三方数据源，清洗入库- 编写数据分析脚本要求： - 热爱技术，对解决具有挑战性问题富有激情，学习能力和求知欲强- 具备强悍的编码能力，内功扎实- 熟悉linux开发环境，熟悉python，毕竟life is short- 有过分布式爬虫开发经验者优先- 熟悉scrapy/redis/mongodb/mysql者优先- 一线大学计算机或相关专业- 阅读英文技术文档无障碍加分：- 有过Android或iOS app开发经验- 有过逆向工程、反编译、破解app经验- 有个人博客、参与开源项目、可提供Github/StackOverflow/知乎等id# 关于我们百观科技（BigOne Lab) 是一家国际化的金融数据技术公司，致力于为全球投资机构挖掘、分析互联网时代所产生的大数据，提供可视化投资决策数据产品。百观正在打造新一代的投资研究产品，目前我们已经为二级市场机构投资者开发出监测多家上市公司运营和财务指标的数据产品，促成了大数据在金融领域的领先应用。百观于2016年获得了真格基金百万美元天使轮融资，2017年获得了华创资本数百万美元Pre A轮融资。我们位于北京，是一个快速成长的国际化创业团队，成员来自百度, Amazon, Bloomberg, Goldman sachs, Sohu, IBM, Alibaba, Yipit Data, iResearch等国际一流技术、数据、与金融公司，团队有1/3成员拥有海外学习与工作经历。团队成员毕业于清华大学、北京大学、纽约大学、芝加哥大学、新加坡国立大学、约翰.霍普金斯大学、耶鲁大学、达特茅斯学院等海内外名校。为了做出最棒的产品，我们需要同样充满好奇心，技艺高超的小伙伴。我们提供：•\t极有竞争力的待遇 + 期权激励•\t有趣的同事•\t超棒的办公环境 # 我们也不喜欢西二旗•\tMacBook Pro，零食饮料，免费午餐，免费口罩...\n",
      "                                我们的团队成员毕业于中美名校，曾任职于Bloomberg, Xiaomi, Baidu, Amazon，Credit Suisse, Goldman Sachs, Chilton Investment, iResearch, Nielsen, Yipit Data等公司。为了做出最棒的产品，我们需要同样充满好奇心，技艺高超的小伙伴。\n",
      "                                        百观科技（BigOne Lab) 是一家国际化的金融数据技术公司，致力于为全球投资机构挖掘、分析互联网时代所产生的大数据，提供可视化投资决策数据产品。百观正在打造新一代的投资研究产品，目前我们已经为二级市场机构投资者开发出监测多家上市公司运营和财务指标的数据产品，促成了大数据在金融领域的领先应用。\n",
      "                                    \n",
      "高级python爬虫工程师 30-40K 集盒平方科技 \n",
      "                                    职位描述：1、负责分布式网络爬虫系统平台的架构设计与开发（如抓取调度，多样化抓取，页面解析和结构化抽取，海量数据存储和读取等）、技术选型；2、研究爬虫策略和防屏蔽规则，解决封账号、封IP、验证码、页面跳转等难点攻克，提升网页抓取的效率和质量；3、利用主流的大数据相关技术，对抓取后的网页数据进行清洗、存储等；并持续优化平台，以便满足各种爬取业务需求；4、把握网络爬虫核心技术研究方向，研究优化算法，提升爬虫系统的稳定性、可扩展性；职位要求：1、全日制本科及以上学历，计算机相关专业，3年及以上爬虫经验；深度参与过至少一个‘分布式网络爬虫系统’的架构设计；2、良好的代码能力，扎实的数据结构和算法功底，有快速迭代、逐步优化的工程项目经验；3、精通爬虫和反爬技术，精通http底层协议；精通深度抓取、动态网页技术抓取、浏览器模拟抓取、APP抓取等技术；4、熟悉分布式系统、多线程，精通scrapy框架以及原理，有开发爬虫框架经验；5、对主流爬虫架构有深入研究，具有成熟爬虫工具的设计及运维经验。6、有很强的学习能力和技术钻研能力，积极主动，思维灵活开放，有良好的沟通能力，善于跨团队合作；\n",
      "                                \n",
      "                                        集盒平方（隐食动力）专注于为中国最优质的餐饮企业，提供以线上运营为核心的一站式商业解决方案，业务涉及本地生活平台运营、传统电商、社交电商、以及自有电商平台的运营，公司现在已获得由一线财务机构领投的数千万元的融资。我们有着实力雄厚的核心团队，对餐饮线上线下运营体系有深刻的理解与认知，核心成员来自阿里\n",
      "                                    \n",
      "高级Python爬虫工程师 (MJ000070) 30-50K·13薪 鲸鱼外教培优 \n",
      "                                    职位描述：1、负责分布式网络爬虫系统平台的架构设计与开发（如抓取调度，多样化抓取，页面解析和结构化抽取，海量数据存储和读取等）、技术选型；2、研究爬虫策略和防屏蔽规则，解决封账号、封IP、验证码、页面跳转等难点攻克，提升网页抓取的效率和质量；3、利用主流的大数据相关技术，对抓取后的网页数据进行清洗、存储等；并持续优化平台，以便满足各种爬取业务需求；4、把握网络爬虫核心技术研究方向，研究优化算法，提升爬虫系统的稳定性、可扩展性；职位要求：1、全日制本科及以上学历，计算机相关专业，4年及以上爬虫经验；2、良好的代码能力，扎实的数据结构和算法功底，有快速迭代、逐步优化的工程项目经验；3、精通爬虫和反爬技术，精通http底层协议；精通深度抓取、动态网页技术抓取、浏览器模拟抓取、APP抓取等技术；4、熟悉分布式系统、多线程，精通scrapy框架以及原理，有开发爬虫框架经验；5、对主流爬虫架构有深入研究，具有成熟爬虫工具的设计及运维经验。6、有很强的学习能力和技术钻研能力，积极主动，思维灵活开放，有良好的沟通能力，善于跨团队合作；\n",
      "                                \n",
      "                                        鲸鱼外教培优成立于2012年，由海内外关注少儿英语教育的家长创建，初期以非营利性组织的方式运营, 依靠良好的口碑积累了上万学员和近千名优秀的北美外教。2017年12月获得山行资本领投的1500万元Pre-A轮融资，2018年1月开始商业化运营，进入快速发展阶段，7月获得清新资本领投、山行资本跟投的千\n",
      "                                    \n",
      "高级Python爬虫工程师 30-40K 鲸鱼外教培优 \n",
      "                                    职位描述：1、负责分布式网络爬虫系统平台的架构设计与开发（如抓取调度，多样化抓取，页面解析和结构化抽取，海量数据存储和读取等）、技术选型；2、研究爬虫策略和防屏蔽规则，解决封账号、封IP、验证码、页面跳转等难点攻克，提升网页抓取的效率和质量；3、利用主流的大数据相关技术，对抓取后的网页数据进行清洗、存储等；并持续优化平台，以便满足各种爬取业务需求；4、把握网络爬虫核心技术研究方向，研究优化算法，提升爬虫系统的稳定性、可扩展性；5、有带人经验（这里的带人指技术指导）职位要求：1、全日制本科及以上学历，计算机相关专业，3年及以上爬虫经验；深度参与过至少一个‘分布式网络爬虫系统’的架构设计；2、良好的代码能力，扎实的数据结构和算法功底，有快速迭代、逐步优化的工程项目经验；3、精通爬虫和反爬技术，精通http底层协议；精通深度抓取、动态网页技术抓取、浏览器模拟抓取、APP抓取等技术；4、熟悉分布式系统、多线程，精通scrapy框架以及原理，有开发爬虫框架经验；5、对主流爬虫架构有深入研究，具有成熟爬虫工具的设计及运维经验。6、有很强的学习能力和技术钻研能力，积极主动，思维灵活开放，有良好的沟通能力，善于跨团队合作；\n",
      "                                \n",
      "                                        鲸鱼外教培优成立于2012年，由海内外关注少儿英语教育的家长创建，初期以非营利性组织的方式运营, 依靠良好的口碑积累了上万学员和近千名优秀的北美外教。2017年12月获得山行资本领投的1500万元Pre-A轮融资，2018年1月开始商业化运营，进入快速发展阶段，7月获得清新资本领投、山行资本跟投的千\n",
      "                                    \n",
      "Python高级爬虫开发工程师 25-35K·15薪 搜狗 \n",
      "                                    职位职责：1、能够准确理解产品需求，负责爬虫平台功能开发和优化；2、负责公司大数据部门的各项数据抓取需求；3、负责Java/Python后台架构搭建，性能方面的优化，解决相关的疑难问题；4、负责研究Python爬虫和后台相关的各种框架或新技术并在程序中进行合理的应用；5、团队技术体系的搭建和完善，推进团队内各项制度流程完善；任职要求：1、有优良的编程风格和习惯，本科及以上学历，计算机相关专业；2、掌握基本数据结构和算法，能够灵活使用编程技巧和设计模式等相关知识；3、扎实的Python开发基础，能够独立承担开发任务；4、能够熟练使用过数据库（如：MySQL），缓存（如：Redis）；5、有强烈的责任心，工作态度严谨，能承担较大工作压力，具备良好的沟通能力和团队合作精神；6、具有Java后台和前端相关开发经验者优先。\n",
      "                                \n",
      "                                        搜狗是中国互联网领先的搜索、输入法、浏览器和其它互联网产品及服务提供商。从2004年8月搜狐公司推出全球首个第三代互动式中文搜索引擎——搜狗搜索以来，历经十载，搜狗搜索已发展成为PC端搜索三强之一，移动搜索排名第二。根据艾瑞咨询2015年8月数据，搜狗PC用户规模达5.21亿\n",
      "                                    \n",
      "Python 爬虫开发工程师 11-20K Stratifyd \n",
      "                                    岗位要求：1）负责开发分布式网络爬虫系统，进行多平台信息的抓取和分析；2）负责网页信息抽取、数据清洗等研发和优化工作；3）负责公司数据仓库平台的 ETL 分析、设计、开发工作 任职资格：1）3年以上爬虫开发经验；2）熟悉 Linux 平台开发，精通 Python 编程；3）精通网页抓取原理及技术，精通正则表达式，从结构化的和非结构化的数据中获取信息；4）熟悉 MySQL、Redis、MongoDB 等数据库；5）大学本科及以上学历 加分项：1）有数据库调优和海量数据存储经验优先；2）有 Solr、ElasticSearch 开发经验者优先；3）具有数据挖掘、自然语言处理、信息检索、机器学习背景者优先\n",
      "                                \n",
      "                                        Stratifyd, Inc.总部位于美国南部金融重镇夏洛特，是全球领先的人工智能（AI）数据分析服务提供商。公司有强大的非结构化数据语义分析能力，致力于推进AI在企业数据分析以及商业智能领域的进步。数据分析是一个高难度、耗时间的过程，读懂大数据背后的故事、分享对的故事给对的人则难上加难。我们的人工\n",
      "                                    \n",
      "高级Python开发工程师／爬虫／Spider 15-25K 智线云 \n",
      "                                    工作内容：1、 基础系统架构、模块、库和组件的研发；2、 分布式海量数据存储和分析系统的研发和调优；3、统计分析、报表、SaaS业务等系统研发；4、CRM、财务结算、工作流等内部管理系统的研发。能力要求：1、至少使用PHP/Java/Python等一门以上语言，开发过不算太小的项目；2、熟悉数据结构、DB、OS、Web开发等相关知识，对Linux相关的各类技术情有独钟；3、有大规模、高性能互联网网站系统相关的设计和开发经验者优先；4、具备良好的学习能力和成长潜力，渴望和团队一起快速成长。\n",
      "                                我们致力于移动互联网营销技术和服务，为客户带来更棒的产品体验、更简单的商业决策，用智能引擎改变客户认知和商业规则，革新传统的工作室方式。智线云工作室是由乐于接受挑战的员工和工程师组成的坚实团队，我们的团队都来自于顶级公司，喜欢突破，渴望挑战，热衷于实践和创造。公司网站：http://zingfront.com\n",
      "                                        【关于公司】智线云专注于大数据、3D云、创意云技术，以“为企业创意赋能”为使命，通过智能技术帮助企业进行广告视频创意制作、广告情报分析以及虚拟形象运营、3D创新教育等多元营销。研发工程师超七成、核心成员来自BAT，是一家正在高速成长扩张中的技术驱动型创意企业。【关\n",
      "                                    \n",
      "Python爬虫算法研发工程师（实习） 150-200/天 Mbyte \n",
      "                                    工作职责：1.负责后台系统框架的研发与维护2.根据产品需求和技术演进，进行程序模块的开发实现工作任职要求1.计算机或相关专业，本科及以上学历；2.熟悉Python编程语言，熟悉正则表达式；3.熟悉软件工程，熟悉常用数据结构和算法；4.熟悉HTML，JavaScript，xpath，css selector，熟悉HTTP协议；5.熟悉scrapy框架；6.熟悉Linux操作系统以及shell脚本；7.熟悉Git版本管理工具；8.英语水平达到CET6或以上；9.工作细心，认真负责，具备较强的学习能力；10.至少实习半年以上，优秀者提供转正机会。\n",
      "                                \n",
      "                                        互联网电商行业\n",
      "                                    \n",
      "Python爬虫算法研发工程师（实习） 150-200/天 Mbyte \n",
      "                                    工作职责：1.负责后台系统框架的研发与维护2.根据产品需求和技术演进，进行程序模块的开发实现工作任职要求1.计算机或相关专业，本科及以上学历；2.熟悉Python编程语言，熟悉正则表达式；3.熟悉软件工程，熟悉常用数据结构和算法；4.熟悉HTML，JavaScript，xpath，css selector，熟悉HTTP协议；5.熟悉scrapy框架；6.熟悉Linux操作系统以及shell脚本；7.熟悉Git版本管理工具；8.英语水平达到CET6或以上；9.工作细心，认真负责，具备较强的学习能力；10.至少实习半年以上，优秀者提供转正机会。\n",
      "                                \n",
      "                                        互联网电商行业\n",
      "                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python爬虫工程师 8-12K 大拿网络科技 \n",
      "                                    岗位职责：1.有过抓取电商网站实际经验；能够独立完成编写爬虫实现对互联网网站、网页内容的爬取、采集和数据抽取等工作； 2.内容提取、处理、结构化、过滤等数据处理，并设计存储结构；3.搭建通用爬虫框架，支持各种定制化爬虫需求的快速实现； 4.攻关抓取过程中的关键技术，优化代理，调度，解析，数据处理等核心模块。 岗位要求: 1.计算机相关专业统招大专以上学历2.精通python或Java语言，1年以上项目开发经验；3.熟悉scrapypyspider nutchHeritrix或其他爬虫框架； 4.有较多的web站点爬取、开发经验； 英语水平优秀者优先5.熟悉网页抓取原理及技术，熟悉基于Cookie的网站登录原理，熟悉基于正则表达式、Xpath、CSS、DOM模型等网页信息抽取技术；\n",
      "                                \n",
      "                                        大拿在2014成立，2017年全新推出Sweet World APP，即\"甜觅世界\"。作为全球旅游购物地图，Sweet World通过3D地图将全球实体店铺和用户直接连接，让用户线上就能在当地购物。 商品直接由国外商家发货至国内，舍去代购，商家标价多少，到手就是多少！\n",
      "                                    \n",
      "Python爬虫实习生 150-200/天 氢元数据 \n",
      "                                    1、熟练使用Python开发2、熟练掌握基于正则表达式、XPath、CSS等网页信息抽取技术3、熟悉最少一种爬虫框架4、每周至少4天工作时间，总实习时间半年及以上\n",
      "                                \n",
      "                                        北京氢元数据信息技术有限公司是行业领先的互联网大数据解决方案提供商。运用人工智能与大数据技术，整合互联网数据，建立一套大数据平台，为企业提供数据决策支持。助力企业品牌管理、市场调研、产品定位、营销决策。氢元数据服务于国内外一流企业，涵盖新能源、金融、汽车、互联网等领域。氢元数据成立于2017年3月1\n",
      "                                    \n",
      "python爬虫工程师(实习) 4-5K 氢元数据 \n",
      "                                    python实习生岗位要求：1、计算机科学、数学、统计学、软件工程或相关理工科专业大专或以上学历；2、熟练掌握至少一门爬虫框架，需有Python编程经验者；3、熟练掌握基于正则表达式、XPath、CSS等网页信息抽取技术；4、熟悉常见反爬机制，验证码识别，IP代理池；5、熟悉应用Selenium+PhantomJS实施动态HTML抓取；6、熟悉应用Ip池、headers认证和cookie等；7、熟练使用http代理工具charles、fiddler等；8、数据结构与经典算法等计算机基础扎实，逻辑思维较强；9、热爱编程，喜欢折腾各类工具，热衷于探寻技术背后的原理；10、责任心较强，做事细致耐心，具有良好的团队合作意识；职位诱惑：实习期不打卡，提供转正机会，技术大牛免费带，提升迅速；行业前景好、发展空间大、隔三差五零食水果提供；\n",
      "                                \n",
      "                                        北京氢元数据信息技术有限公司是行业领先的互联网大数据解决方案提供商。运用人工智能与大数据技术，整合互联网数据，建立一套大数据平台，为企业提供数据决策支持。助力企业品牌管理、市场调研、产品定位、营销决策。氢元数据服务于国内外一流企业，涵盖新能源、金融、汽车、互联网等领域。氢元数据成立于2017年3月1\n",
      "                                    \n",
      "Python爬虫开发工程师 10-18K 企名片 \n",
      "                                    1、工作内容负责公司爬虫系统开发2、公司产品属于垂直搜索，在这里有很多的技术可以学习提高3、公司方向是大数据和人工智能，有志于在这方面有深入学习非常适合4、要求对mysql python熟练\n",
      "                                普惠企业大数据，这里有足够的发展空间，初创企业，各种挑战等你来！\n",
      "                                        公司目前已获得数千万融资，投资机构：蓝驰创投、晨兴资本、初心资本等投资方。【企名片】是一家面向投资人及创业者的创投服务平台，致力于通过大数据及人工智能等技术让投资更容易，更快捷。在企名片，投资人可以直接通过选择的关注领域行业，更及时的发现自己关注的行业动态，更快的发现自己的感兴趣的项目，\n",
      "                                    \n",
      "Python爬虫⼯工程师 18-35K 一点资讯 \n",
      "                                    工作职责：1、负责分布式⽹网络爬⾍虫系统的设计和开发，进⾏行行多平台多信息的抓取和分析⼯工作；2、负责解决各种反爬取问题，App爬取分析等；3、负责数据抽取、清洗、去重等⼯工作，提升平台的抓取效率；4、负责数据存储、管理理和数据服务搭建；工作要求：计算机相关专业本科或以上学历精通python语⾔言，熟练使⽤用MySQL/Redis/MongoDB等数据库；精通⽹网⻚页抓取原理理及技术，精通正则表达式，熟悉html/dom/xpath，熟练从结构化和⾮非结构化的数据中获取信息；精通⼀一种开源爬⾍虫框架，如scrapy、webmagic、nutch、heritrix等，有开发爬⾍虫框架经验优先；\n",
      "                                \n",
      "                                        一点团队一点科技团队由一群来自硅谷和国内一流互联网公司的顶尖人才组成，主要技术负责人有着多年在硅谷知名互联网公司核心部门的研发和管理经验。一点团队在搜索、推荐、个性化、数据挖掘、机器学习以及网页/移动端开发等领域均拥有很深厚技术积累和优秀的口碑。一点产品“一点资讯\n",
      "                                    \n",
      "Python爬虫工程师 11-20K 曜辉科技 \n",
      "                                    职责描述：1、负责网络爬虫程序架构设计，开发及维护;2、负责对目标网站进行数据爬取分析，找到最优化的爬取策略。 3、负责优化抓取程序的监控和报警，技术难点的攻克； 4、负责大规模数据爬虫的性能优化工作；5、负责统计分析和报表；任职要求：1、计算机本科相关专业，3年经验，熟悉软件架构，精通python, 2年以上爬虫工作经验者优先；2、精通网页抓取原理及技术，精通正则表达式，熟练从结构化和非结构化的数据中获取信息；3、熟练使用常用数据结构，理解http,html,DOM,xpath,精通scrapy优先;4、熟悉数据库性能优化与常见缓存技术与策略,有大规模、高性能互联网网站系统相关的设计和开发经验者优先；5、具备优秀的逻辑思维能力,有强烈上进心,善于分析问题/解决问题,学习适应能力强;\n",
      "                                \n",
      "Python爬虫架构 20-40K 北京南哲科技 \n",
      "                                    岗位职责：1、负责设计和开发分布式爬虫系统和框架，带领团队完成完善的爬虫团队体系2、实现大规模文本、图像数据的抓取及数据清洗工作；3、与大数据工程师对接，完成实时数据与离线数据的爬取与对接。任职要求：1、能够解决封账号、封IP采集等问题，解决网页抓取、信息抽取等问题；2、有验证码破解，反爬，分布式爬虫架构经验者优先；3、从结构化和非结构化数据中解析数据，能够总结分析不同网站，网页的结构特点及规律；4、熟悉Appium、Selenium、PhantomJS 、WebDriver等技术；5、熟悉Mysql、Redis、Nosql等数据库；6、熟悉常见的数据结构知和基本的算法复杂度概念。与我们有共同的基因：1、拥有敏锐的产品嗅觉，有强烈的好奇心，对业务、对产品有深厚的兴趣，对技术驱动业务有强烈的激情；2、结果导向，能够通过数据分析驱动产品的优化；3、自我驱动，勤奋而善于思考，愿意不断挑战和提升自己；4、喜欢钻研，乐于尝试新技术，不断拓展个人技术栈。\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "Python 抓取/爬虫技术专家 20-40K 北京南哲科技 \n",
      "                                    岗位职责：1、负责设计和开发分布式爬虫系统，进行多平台多终端信息的抓取和分析；2、实现大规模文本、图像数据的抓取及数据清洗工作；3、与大数据工程师对接，完成实时数据与离线数据的爬取与对接。任职要求：1、能够解决封账号、封IP采集等问题，解决网页抓取、信息抽取等问题；2、有验证码破解，反爬，分布式爬虫架构经验者优先；3、从结构化和非结构化数据中解析数据，能够总结分析不同网站，网页的结构特点及规律；4、熟悉Appium、Selenium、PhantomJS 、WebDriver等技术；5、熟悉Mysql、Redis、Nosql等数据库；6、熟悉常见的数据结构知和基本的算法复杂度概念。与我们有共同的基因：1、拥有敏锐的产品嗅觉，有强烈的好奇心，对业务、对产品有深厚的兴趣，对技术驱动业务有强烈的激情；2、结果导向，能够通过数据分析驱动产品的优化；3、自我驱动，勤奋而善于思考，愿意不断挑战和提升自己；4、喜欢钻研，乐于尝试新技术，不断拓展个人技术栈。\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "Python 高级爬虫工程师 15-30K 北京南哲科技 \n",
      "                                    岗位职责：1、负责设计和开发分布式爬虫系统，进行多平台多终端信息的抓取和分析；2、实现大规模文本、图像数据的抓取及数据清洗工作；3、与大数据工程师对接，完成实时数据与离线数据的爬取与对接。任职要求：1、能够解决封账号、封IP采集等问题，解决网页抓取、信息抽取等问题；2、有验证码破解，反爬，分布式爬虫架构经验者优先；3、从结构化和非结构化数据中解析数据，能够总结分析不同网站，网页的结构特点及规律；4、熟悉Appium、Selenium、PhantomJS 、WebDriver等技术；5、熟悉Mysql、Redis、Nosql等数据库；6、熟悉常见的数据结构知和基本的算法复杂度概念。与我们有共同的基因：1、拥有敏锐的产品嗅觉，有强烈的好奇心，对业务、对产品有深厚的兴趣，对技术驱动业务有强烈的激情；2、结果导向，能够通过数据分析驱动产品的优化；3、自我驱动，勤奋而善于思考，愿意不断挑战和提升自己；4、喜欢钻研，乐于尝试新技术，不断拓展个人技术栈。\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "python 爬虫验证码处理工程师 10-15K 北京南哲科技 \n",
      "                                    1、熟练掌握Python和Java语言，对相关图形算法有深刻研究；2、熟练使用OpenCV、SVM等相关工具和模型；3、对GPU计算有一定的研究和了解；4、熟练使用tensorflow进行图形训练和分析；5、做过多种验证码的算法分析和研究。\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "Python 爬虫 验证码破解负责人 20-40K 北京南哲科技 \n",
      "                                    岗位职责：1、验证码相关问题攻克2、Js逆向任职要求：1、专业学历均不限2、精通Js逆向 擅长破解各种反爬 反作弊系统3、精通Python/node.js/java等任何语言4、精通各种前端技术(javascript/html/css)5、熟悉浏览器开发与内核相关知识硬性条件：擅长破解各种验证码（不限于滑块，点选，谷歌验证码）\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "Python爬虫工程师Leader 20-40K 北京南哲科技 \n",
      "                                    岗位职责：1、负责设计和开发分布式爬虫系统，进行多平台多终端信息的抓取和分析，带领团队完成任务2、实现大规模文本、图像数据的抓取及数据清洗工作；3、与大数据工程师对接，完成实时数据与离线数据的爬取与对接。任职要求：1、能够解决封账号、封IP采集等问题，解决网页抓取、信息抽取等问题；2、有验证码破解，反爬，分布式爬虫架构经验者优先；3、从结构化和非结构化数据中解析数据，能够总结分析不同网站，网页的结构特点及规律；4、熟悉Appium、Selenium、PhantomJS 、WebDriver等技术；5、熟悉Mysql、Redis、Nosql等数据库；6、熟悉常见的数据结构知和基本的算法复杂度概念。与我们有共同的基因：1、拥有敏锐的产品嗅觉，有强烈的好奇心，对业务、对产品有深厚的兴趣，对技术驱动业务有强烈的激情；2、结果导向，能够通过数据分析驱动产品的优化；3、自我驱动，勤奋而善于思考，愿意不断挑战和提升自己；4、喜欢钻研，乐于尝试新技术，不断拓展个人技术栈。\n",
      "                                每月团队聚餐，季度旅行团建，年度出国团建，福利待遇非常好！零食，夜宵不限量，团队技术氛围很强\n",
      "Python爬虫工程师 10-18K BJUED \n",
      "                                    岗位职责：1、负责设计和开发分布式网络爬虫系统，进行多平台信息的抓取和分析2、负责网页信息抽取、数据清洗等研发和优化工作3、负责抓取数据的深度提取和挖掘4、参与爬虫核心算法的策略优化研究，提升网页抓取的效率和质量。任职资格： 1、熟悉linux平台开发，精通Python开发语言2、精通Scrapy-redis，精通网页抓取原理及技术，精通正则表达式，从结构化的和非结构化的数据中获取信息3、熟悉MySQL、MongoDB，有过数据库调优和海量数据存储经验优先4、具有搜索相关技术研发经验者优先\n",
      "                                我们规模不大，但是客户遍及全球；我们都很年轻，但是获奖无数；我们扎根底层研发，保持着纯技术的傲娇；我们热爱创新，脑洞无限。我们是少有的不玩儿概念，执着于技术的团队。加入我们，你会感受到迷上工作的快乐。我们的公司更像一个家，有能让你成就感十足的创新工作，当然也少不了办公室那杯香浓拿铁。\n",
      "                                        BJUED，成立于2012年，位于北京。我们与我们的伙伴一起，在服务中坚持探索 “创新” 与 “可持续” 的模式。通过技术创新与经验分享，我们逐步形成了以自有团队为核心，众多行业专家组成的协作体模式，为客户提供优质解决方案和深度技术咨询服务。U\n",
      "                                    \n",
      "Python爬虫开发工程师 15-30K 谷德环宇科技 \n",
      "                                    职责：1). 负责快速搭建爬虫的框架与环境（数据库，分布式爬虫框架）2). 爬虫代码开发，对采集的资讯信息的清洗与过滤（停止词库与关键词库的完善）以及整理分析（分词算法即关键词的提取，文章内容的摘要提取）3). 保证爬虫系统持续稳定，高效的运行（每日采集数据量持续1千万以上，数据存储合理设计与优化）需具备的条件及要求：1). 从事过信息采集与抓取工作至少一年以上, 有独立完成爬虫项目的经验；2). 能独立完成采集数据库安装与部署,采集开发环境的安装与部署；3). 有丰富的反爬实际工作经验，有丰富的分布式爬虫架构与部署经验。4). 有数据清洗的相关工作经验，熟练掌握Xpath和正则。5). 有过中文和英文分词经验;6). 有过APP抓取项目经验者优先;7). 有过大规模数据(单日信息抓取量500万以上)抓取经验优先。为便于提升沟通效率，节省彼此的时间与精力，务请对照以上岗位要求与任务，客观地评估自己。\n",
      "                                \n",
      "                                        公司创立于2017年，资深互联网连续创业团队，致力于区块链行业服务及基础技术架构研发创新，已获得千万人民币风险投资。\n",
      "                                    \n",
      "python开发工程师（爬虫） 15-30K 36氪集团 \n",
      "                                    职位描述：职位职责： 1、负责构建分布式爬虫架构，智能抓取系统设计与研发（如抓取调度，多样化抓取，页面解析和结构化抽取，海量数据存储和读取等）。 2、实现大规模数据的抓取、解构，去重、分类，垃圾过滤，质量识别等工作 。3、抓取策略算法的更新维护，以及确保数据抽取准确、高效。 职位要求： 1、具备较强的编码能力，熟悉python、熟悉基本的数据结构和算法、熟悉 linux 开发环境。2、熟悉常用爬虫框架中的一种或多种，如Scrapy、pyspider等。3、熟悉HTML、JS、正则表达式、Jsoup或jQuery、XPath等，从结构化和非结构化数据中解析数据；4、具备一定的反爬取经验。5、有验证码破解、数据挖掘经验者优先； 6、有分布式爬虫架构、信息抽取、文本分类相关经验优先。7、有大规模分布式海量数据处理经验优先（如Hadoop/Hbase/Spark/Strom/Flink等）。8、有开源作品优先。\n",
      "                                \n",
      "                                        36氪集团简介36氪集团的愿景是打造国际领先的科技创新创业综合生态服务体系。其使命是为中小微及科技创新企业解决“曝光难、办公难、融资难”的问题，提供包括媒体曝光、办公场地及相关的配套服务、融资对接等全方位一站式服务。中小微及科技创新企业是国家经济活力的主要构成\n",
      "                                    \n",
      "Python爬虫工程师 8-15K 创数云天 \n",
      "                                    职位描述1、 参与公司大数据平台/核心项目的数据采集系统的设计、开发；2、 负责设计开发数据采集策略和防屏蔽规则，提升数据采集的效率和质量；3、 负责根据系统数据处理流程以及业务功能需求，进行核心算法的设计与开发；4、 负责大数据平台/核心项目的数据采集。任职要求1、 2年以上爬虫项目经验；本科以上学历，211或985院校毕业优先；2、 掌握Python技术,熟悉常用爬虫框架中的一种或多种，如Scrapy框架或其他的Web scraping framework；3、 熟悉HTML/JavaScript/CSS/xpath/url/Ajax/xml等Web技术,熟悉HttpClient、jsoup、WebDriver、phantomjs等工具;4、 熟悉linux系统，熟悉postgres、redis，nosql等数据技术,熟悉Internet基本协议（如TCP/IP. HTTP等） ；5、 有分布式爬虫架构设计经验者优先；;6、 熟悉MVC架构，熟悉使用Django、Tornado、Flask、web.py等至少一个开发框架；7、 做事有责任心、有想法，热爱技术，喜欢钻研；对开源项目有浓厚的兴趣。\n",
      "                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python爬虫开发实习 3-6K 中科院计算所 \n",
      "                                    1、计算机相关专业硕士以上学历2、熟练掌握python/Java开发3、熟悉Python爬虫的构建4、具备机器学习和深度学习基础，熟悉Tensor Flow/pytorch框架，具备扎实的数学基础5、熟悉numpy,scipy等科学计算模块，有相关开发经验者优先;6、熟悉MySQL或neo4j数据库系统\n",
      "                                \n",
      "                                        中国科学院计算技术研究所（简称计算所）创建于1956年，是中国第一个专门从事计算机科学技术综合性研究的学术机构。计算所研制成功了我国第一台通用数字电子计算机，并形成了我国高性能计算机的研发基地，我国首枚通用CPU芯片也诞生在这里。计算所是我国计算机事业的摇篮。伴随着计算所的发生发展，先后\n",
      "                                    \n",
      "python爬虫工程师 10-20K 北京普创智通 \n",
      "                                    职责描述：1. 负责多平台信息爬取和页面内容的提取分析；2. 优化抓取策略，充分利用带宽资源，避免限制；3. 分析爬虫系统的技术缺陷，对策略架构做出合理地调整和改进。任职要求：1、全日制本科及以上学历，计算机相关专业；2、熟悉http协议，了解常见header、状态码，了解Ajax请求；3、扎实的Python基础，一年以上Python爬虫项目开发经验；4、熟悉网页抓取原理及技术、正则表达式、xpath，能够从结构化的和非结构化的数据中获取信息；5、至少熟悉一种关系型数据库，如MSSQL、Mysql、Oracle等；6、思维敏捷，逻辑清晰，有较强的分析和解决问题的能力；7、有良好的沟通能力、团队协作精神；良好的编码习惯，重视且能独立撰写技术文档。\n",
      "                                \n",
      "Python开发工程师 15-25K 中国指数研究院 \n",
      "                                    Python爬虫工程师岗位职责：1、 负责分布式网络爬虫系统的架构设计与开发2.、负责破解各类反爬机制，爬虫系统的优化，监控和报警，提高爬虫项目的稳定性和抓取效率3、 负责抓取信息的抽取、数据清洗、数据存储、数据分析等研发工作能力要求：1、统招本科及以上学历，计算机相关专业，3年以上爬虫项目经验2、熟悉Python语言，有扎实的算法功底和代码实现能力，具备良好的分析问题、解决问题能力3、有丰富的分布式爬虫开发经验，熟悉常用的爬虫框架，需要能够  独立  解决主流网站常用的反爬措施，破解反爬经验要丰富5、熟悉主流关系型与非关系型数据库，例如：MySQL、Redis、Mongodb，熟悉linux操作系统6、具有高度的责任感，工作积极主动、学习能力强、善于总结，有规范化文档编写良好习惯\n",
      "                                \n",
      "                                        中国指数研究院（即搜房控股研究集团）于2019年6月11日在美国纳斯达克成功上市。中国指数研究院（网址industry.fang.com）是目前中国最大的房地产专业研究机构，依托搜房网300多个城市建立了中国信息最全的房地产专业数据库，长期致力于房地产市场研究。中国指数研究院整合了中国房地产指\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.zhipin.com/job_detail/?query=python%E7%88%AC%E8%99%AB&city=101010100&industry=&position='\n",
    "page_text = requests.get(url,headers=headers).text\n",
    "#数据解析\n",
    "tree = etree.HTML(page_text)\n",
    "li_list = tree.xpath('//div[@class=\"job-list\"]/ul/li')\n",
    "for li in li_list:\n",
    "#     需要将li表示的局部页面源码数据中的相关数据进行提取\n",
    "#     如果xpath表达式被作用在了循环中,表达式要以./或者.//开头\n",
    "    detail_url = 'https://www.zhipin.com'+li.xpath('.//div[@class=\"info-primary\"]/h3/a/@href')[0]\n",
    "    job_title = li.xpath('.//div[@class=\"info-primary\"]/h3/a/div/text()')[0]\n",
    "    salary = li.xpath('.//div[@class=\"info-primary\"]/h3/a/span/text()')[0]\n",
    "    company = li.xpath('.//div[@class=\"info-company\"]/div/h3/a/text()')[0]\n",
    "    #对详情页的url发请求解析出岗位职责\n",
    "    detail_page_text = requests.get(detail_url,headers=headers).text\n",
    "    tree = etree.HTML(detail_page_text)\n",
    "    job_desc = tree.xpath('//div[@class=\"text\"]//text()')\n",
    "    job_desc = ''.join(job_desc)\n",
    "    \n",
    "    print(job_title,salary,company,job_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另一种较为通用的xpath表达式的使用形式:管道符在xpath表达式中的应用,用于解析不规则布局的页面数据\n",
    "- 爬取糗事百科的段子内容和作者名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-小门神～\n",
      " \n",
      "\n",
      "\n",
      "那天媳妇生日，我提前下班计划计划。等我准备好惊喜居然停电了，媳妇回家一推开门，看到我准备的生日惊喜，惊呆了:黑暗中，小圆桌上摆着两只大号的白蜡烛，中间一个八宝粥罐子里插着三根荧光棒！桌子上摆满了花生瓜子，烧鸡烧鸭烧鹅，有鲜花，还有一沓从银行取出的钞票，墙上挂着一张她的自画像！我把桌子上的钞票交给她，霸气侧漏说:拿去，随便花。她嘴角使劲抽了抽:你特么，这是打算给我上供啊？纸钱你都准备好了！！！\n",
      "\n",
      "\n",
      "\n",
      "无书斋主\n",
      " \n",
      "\n",
      "\n",
      "昨晚下班回家时小区几个小孩打架，我大喝一声，吓得他们落荒而逃……吃完晚饭想去买个西瓜，走到小孩打架那，一群小孩冒出来围住了我:你个老东西，多管闲事，今天不给我们讲好话打死你……就是这个伯伯救的我，我们上……刚打架弱势的小哥哥带了一群小朋友冲了过来……又是一场混战……我正构思怎么写成段子时，来了几个大人:正哥，小朋友打架也不拉下，还看得有味啊？！……我……我都差点被打了，还拉个毛线！……\n",
      "\n",
      "\n",
      "\n",
      "捉女妖的天师\n",
      " \n",
      "\n",
      "\n",
      "大儿子想学乐器，这是好事可以陶冶情操，至少比我喝酒的时候只会吹牛要高雅的多！他是这么回答的:等我学会了可以去参加红白喜事\n",
      "\n",
      "\n",
      "\n",
      "正版煮茶\n",
      " \n",
      "\n",
      "\n",
      "我以前上大学，临近暑假，我爸打电话给我：“臭小子，暑假回来不？”我：“不回了，我想在这找份暑期工！”我爸声音颤抖：“那就好，那你过年也别回来了！”   血浓于水，我爸到底是想我了啊……我感动的湿了眼眶：“别生气啊，爸！”我爸：“我没生气，我昨天把你房间租出去了，跟人家签了一年的合同，多少能赚点烟钱...”\n",
      "\n",
      "\n",
      "匿名用户 \n",
      "\n",
      "\n",
      "麻痹的，一连发了十多个都不给过，从此我审帖   通通不给过\n",
      "\n",
      "\n",
      "\n",
      "歌歌哒f\n",
      " \n",
      "\n",
      "\n",
      "上学那会看着舍友们都谈恋爱，很是羡慕，于是缠着一哥们叫他把他女朋友的闺密介绍给我，哥也很爽快，说去和他女盆友说说，经过他两的不断努力那个女孩终于答应和我约会了，出门时哥们一脸坏笑道：知道怎么做吗……我整理着发型道：我懂，女孩嘛都是小孩子，要哄，放心吧，比了一个ok的手势就出发了……来到约好的地方，女孩早已经等在那了，她打了一把太阳伞背对着湖靠在栏杆上，我有些不好意思走过去道：来得这么早啊，她“嗯”了一声，于是我俩陷入了沉默，当我也背对着湖靠在栏杆上默数了几个过路人的脚后，心想一直\n",
      "…\n",
      "查看全文\n",
      "\n",
      "爆笑菌boy\n",
      " \n",
      "\n",
      "\n",
      "出差几天回到家，发现烟灰缸里多了几个高档的烟头。多亏平时喜欢看侦探小说，有着缜密的推理思维，由此我断定：1.老婆背着我在学抽烟；2.她藏的好烟应该还剩几根。。。等她回来，看我怎么收拾她，平时就让我抽五块的，我非得弄出几支好的尝尝，这个败家老娘们儿。。。\n",
      "\n",
      "\n",
      "\n",
      "正版煮茶\n",
      " \n",
      "\n",
      "\n",
      "我第一次开车回老家，乡亲们很热情。回城的时候，我开车出村口，还有乡亲追上来送别。我不禁热泪盈眶，打开车窗听到他边追边喊：“快停车！我晒在你车上的萝卜干还没收...”\n",
      "\n",
      "\n",
      "\n",
      "哈和嗨\n",
      " \n",
      "\n",
      "\n",
      "我 一 女 同 学 ，毕 业 后 去 家 大 公 司 面 试 ，竞 争 激 烈 ，最 后 剩 下 她 和 另 一 女 士 。两 人 都 非 常 优秀 ，面 试 官 犹 豫 叫 她 们 明 天 再 来 面 试 。临 走 之 前 ，她 捡 起 了 地 上 的 碎 纸 屑 。所 以 说 ，细 节 决 定 成 败 ，正 是 这 一 不 经 意 的 弯 腰 ，恰 巧 被 路 过 的 CEO 看 在眼 里 ，透 过 衣 领 ，发 现 她 的 胸 特 别 小 ，当 机 立 断 录 取 了 另 一 个 女 的\n",
      "…\n",
      "查看全文\n",
      "\n",
      "醉压花丛蜜粘身\n",
      " \n",
      "\n",
      "\n",
      "说个曾经追过的一个女孩纸的，她是资深糗友，某天我和她下班后在外面休息，她用手机看糗百，我在玩天天酷跑！～～不割留用～～这时路上来了两个男孩，坐在我们对面的一个女孩和他们认识，就和他们对话，男:“你分配到几楼上班？”女:“我在二楼，你们呢？”男:“我在三楼他在一楼（指着旁边另一个男孩）”接着又说:“我在你上面，他在你下面”！本着糗百精神，某女小声说了句“邪恶哦！”我问:“你说什么？”她急忙说:“没什么！！！”我停下来想了想，拉着她的马尾辫说:“是你邪恶了吧！别以为我听不出来！”话音刚落，只见周围所\n",
      "…\n",
      "查看全文\n",
      "\n",
      "小苏溪\n",
      " \n",
      "\n",
      "\n",
      "晚上下班超市买东西赶上某芙巧克力做促销，想着推出新产品就过去尝尝。促销员热情的打开一块巧克力放容器里融化，咦～难道了更好吃吗？正想呢促销员示意手伸出来，然后挖一勺巧克力倒我手上，涂抹。我愣住了，促销员看我疑惑的表情微微一笑:某芙~纵享丝滑！滑不？\n",
      "\n",
      "\n",
      "\n",
      "tonoon\n",
      " \n",
      "\n",
      "\n",
      "我大学一个外教是个中国通，汉语贼6，他老婆是中国人，是个英文老师，英文贼6，结果就是他俩说着说着就混了，他和她老婆说中文，他老婆和他说英文，留下我们一众大眼瞪小眼，不知道到底谁是老外\n",
      "\n",
      "\n",
      "\n",
      "屌爆了所以是女生\n",
      " \n",
      "\n",
      "\n",
      "妈妈，你现在抱抱我可以吗？我还是没有特别高，等我长到和警察叔叔一样高的时候，我就要去做工了，去挣钱钱给你买面膜买口红，买包包了，所以我现在没有很高的时候，你要抱抱我。三岁的宝宝天真的对我说[doge][doge][doge]这理由，都找不到反驳的话\n",
      "\n",
      "\n",
      "\n",
      "小爷复姓欧阳诺一\n",
      " \n",
      "\n",
      "\n",
      "一个哥们有点二，大学毕业当了警察，前段时间抓了一个传销的头子，老警察都不想审，因为证据确凿，就让这哥们去了，结果审到大半夜，他和嫌疑犯一起失踪了。后来把人抓回来才知道，这货居然被传销头子说服了...\n",
      "\n",
      "\n",
      "\n",
      "幸福的妹纸@，，，…\n",
      " \n",
      "\n",
      "\n",
      "我能说我前凸后翘，唯一的缺点就是个不高，也是致命的缺点，人也属于清纯型的，够糗吗，\n",
      "\n",
      "\n",
      "\n",
      "捉鱼不抓虾\n",
      " \n",
      "\n",
      "\n",
      "记得结婚前的那些岁月，每当过节的时候，媳妇总是说，老公，你给我买个项链，我就让你嘿嘿嘿。现在结婚已多年，到过节的时候，我都是早早准备好礼物，只求能放过我。今夜是七夕，礼物没让她满意，看着手里六味地黄丸，我狠狠的吞下一把！\n",
      "\n",
      "\n",
      "\n",
      "莫纳的世界\n",
      " \n",
      "\n",
      "\n",
      "昨晚欠我钱许久没联系的一个哥们竟然给我微信运动点了个赞，让我平静的心又一次掀起了波澜……为什么要这样折磨我？！！！\n",
      "\n",
      "\n",
      "\n",
      "tonoon\n",
      " \n",
      "\n",
      "\n",
      "作为年轻的中学班主任，我有时会在其他课上坐到后排监督课堂纪律～直到有个新来的眼镜实习老师～这货见我上课一直低头，喊我起来回答问题……一时语塞……竟然开始训起我了……全班那个爆笑啊，她很有成就感似的根本停不下来……直到我默默亮出工作卡……\n",
      "\n",
      "\n",
      "\n",
      "半截烟云\n",
      " \n",
      "\n",
      "\n",
      "同事生了个女儿，今天发朋 友 圈郁闷地说没一个人觉得女儿像她，都说像她老公，我说那不是更好，你老公都不用怀疑孩子不是他亲生的。同事更郁闷了:可长得像他太丑了，我看一个就够了，往后还得看两个……\n",
      "\n",
      "\n",
      "\n",
      "莫方，抱紧我\n",
      " \n",
      "\n",
      "\n",
      "真事不割，记得刚上大学的时候，有个奇葩室友都要把新买的衣服还有被子拿到楼顶晒晒，声称“杀杀菌！用着才卫生！”～～～然后下午去收衣服的时候，没错！太特么干净了，全不见了，一件衣服都没留下！～留下他一个人在风中凌乱～～～\n",
      "\n",
      "\n",
      "\n",
      "花落彼岸～任逍遥\n",
      " \n",
      "\n",
      "\n",
      "今天早上我在小吃店吃了七个包子，一碗混沌和两个鸡蛋，边上的一个女孩吃惊的看着我，跟她边上的女伴说:“这家伙真能吃，在拍西游可以把他加上，让他做唐僧的第五个徒弟，你看有沙僧唐僧在加上这个吃僧。额你们说悄悄话能不能小点声呢\n",
      "\n",
      "\n",
      "\n",
      "前女友腿又细又长\n",
      " \n",
      "\n",
      "\n",
      "大学时有一回同学聚会喝断片了，第二天在酒店醒来，有个女同学给我发了条消息，说就当昨晚什么也没发生，关键是我什么也没记住啊。后来还是把这件事告诉女友了，结果她说恶作剧罢了，她之前有一回也是喝断片了，结果全班男生都说要对她负责。\n",
      "\n",
      "\n",
      "\n",
      "172的小菇凉\n",
      " \n",
      "\n",
      "\n",
      "刚刚闺蜜得意地跟我说，昨天心情不好，想找个理由骂老公，抬头一看，他又是做饭又是扫地的，就往花瓶里丢了100块钱，想诬陷他藏私房钱。然后把花瓶往地下一摔。尼玛，居然甩出了526块6毛钱！\n",
      "\n",
      "\n",
      "\n",
      "薛家小姐姐\n",
      " \n",
      "\n",
      "\n",
      "台风来了，急忙去阳台收拾凉洗的衣服，着急中把一条内内落入楼下邻居阳台上，这是老公最喜欢的一条，于是壮着胆子去楼下敲开门，男主人在家，我说明来意并解释说:“这个是新买的还没穿，要不是新买的我就不要了”他盯着我说:“闻着不是新买的。”我……\n",
      "\n",
      "\n",
      "\n",
      "坐在墙头等你来\n",
      " \n",
      "\n",
      "\n",
      "出去学习睡了半个月酒店，腰疼半个月，回到家一睡硬板床啥事都没了，唉。。。。。屌丝真心伤不起啊\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',\n",
    "}\n",
    "url = 'https://www.qiushibaike.com/text/page/4/'\n",
    "page_text = requests.get(url,headers=headers).text\n",
    "\n",
    "tree = etree.HTML(page_text)\n",
    "div_list = tree.xpath('//div[@id=\"content-left\"]/div')\n",
    "for div in div_list:\n",
    "    author = div.xpath('./div[1]/a[2]/h2/text() | ./div[1]/span[2]/h2/text()')[0]\n",
    "    content = div.xpath('.//div[@class=\"content\"]/span//text()')\n",
    "    content = ''.join(content)\n",
    "    print(author,content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文乱码处理的问题\n",
    "- 爬取http://pic.netbian.com/4kmeishi/的图片和图片名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定一个通用的url模板\n",
    "url = 'http://pic.netbian.com/4kmeishi/index_%d.html'\n",
    "\n",
    "for page in range(1,3):\n",
    "    if page == 1:\n",
    "        new_url = 'http://pic.netbian.com/4kmeishi/'\n",
    "    else:\n",
    "        new_url = format(url%page)\n",
    "    response =  requests.get(new_url,headers=headers)\n",
    "    #response.encoding = 'utf-8'\n",
    "    page_text = response.text\n",
    "    tree = etree.HTML(page_text)\n",
    "    li_list = tree.xpath('//*[@id=\"main\"]/div[3]/ul/li')\n",
    "    for li in li_list:\n",
    "        img_src = 'http://pic.netbian.com'+li.xpath('./a/img/@src')[0]\n",
    "        img_name = li.xpath('./a/b/text()')[0]\n",
    "        img_name = img_name.encode('iso-8859-1').decode('gbk')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
